{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a6087d-6269-4704-a254-e920302e4161",
   "metadata": {},
   "source": [
    "# RNN\n",
    "We used an RNN, this time on the preprocessed text of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1632899a-43ec-4a56-8ab7-7cbfda0261e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb639e8-ea1b-4a81-9623-b577edfa149f",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "818d89ff-7698-463a-beca-cd9e4025f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2a732852-8879-4969-85e0-124f78473f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense,Embedding, Bidirectional, Attention, LSTM, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bf4414f-5852-462e-a703-d0fe892831e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5974b777-ee4d-4eeb-8c82-e68e0f04340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13d0f8-108e-420a-8899-204563a7db7d",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5e1cf7a-2136-474d-86fc-746c26e98500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "## CHECK GPU\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb24c5ad-aaf6-4354-8641-19a5b3c2ddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6952b0b0-4eb8-4806-8995-33ab9e63e578",
   "metadata": {},
   "source": [
    "### Functions\n",
    "The first function serves to convert a string into a Python object, the second to create text from tokens, and the last to split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6350537c-20f4-4443-a852-3760d99e0144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS DEFINITION\n",
    "\n",
    "#READ SPLIT TOKENS\n",
    "def safe_literal_eval(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error parsing value {val}: {e}\")\n",
    "        return val  # Return the original value if there is an error\n",
    "\n",
    "#MERGE TOKENS AS A WHOLE TEXT\n",
    "def join_tokens(token_list):\n",
    "    if isinstance(token_list, list):\n",
    "        return ' '.join(token_list)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "#SPLIT TRAIN + TEST 80-20\n",
    "def split_train_test(df, label_name):\n",
    "    train, test= train_test_split(df, test_size=0.2, stratify=df[label_name],random_state=42)\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c94c75-cf39-4816-bc96-317a8d2f6523",
   "metadata": {},
   "source": [
    "### Function metrics\n",
    "Here we have defined the various functions to calculate the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73e59c91-abb7-4179-ae48-144a4528ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def precision_m(y_true, y_pred):     \n",
    "    y_pred = tf.nn.sigmoid(y_pred)  # Apply sigmoid to get probabilities   \n",
    "    y_pred = K.round(y_pred)  # Convert probabilities to 0 or 1    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))     \n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))     \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())     \n",
    "    return precision\n",
    "\n",
    "# Custom metric for recall\n",
    "def recall_m(y_true, y_pred):\n",
    "    y_pred = tf.nn.sigmoid(y_pred)  # Apply sigmoid to get probabilities\n",
    "    y_pred = K.round(y_pred)  # Convert probabilities to 0 or 1\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "# Custom metric for F1 score\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "def false_negative_rate_m(y_true, y_pred):\n",
    "    # Apply sigmoid to get probabilities\n",
    "    y_pred = tf.nn.sigmoid(y_pred)\n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred = K.round(y_pred)\n",
    "    # Calculate False Negatives\n",
    "    false_negatives = K.sum(K.cast(y_true, dtype='float32') * (1 - y_pred))\n",
    "    # Calculate True Positives + False Negatives (total actual positives)\n",
    "    possible_positives = K.sum(K.cast(y_true, dtype='float32'))\n",
    "    # Calculate False Negative Rate\n",
    "    fnr = false_negatives / (possible_positives + K.epsilon())\n",
    "    return fnr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d52b4-ebd9-43f8-83c4-ac08c84be47d",
   "metadata": {},
   "source": [
    "###  Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0743baa8-3261-49a3-81a6-13bc04c42c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData=pd.read_csv(\"../Dataset/datiClean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3cb878e4-99bf-4d45-9c25-611d9ca6d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData=CleanData[[\"clean_review\",\"is_spoiler\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d5b6c2b-ad3c-4340-91a2-602c8590a771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_review</th>\n",
       "      <th>is_spoiler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['oscar', 'year', 'shawshank', 'redemption', '...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['shawshank', 'redemption', 'without', 'doubt'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['believe', 'film', 'best', 'story', 'ever', '...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['yes', 'spoiler', 'film', 'emotional', 'impac...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['heart', 'extraordinary', 'movie', 'brilliant...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573908</th>\n",
       "      <td>['go', 'wise', 'fast', 'pure', 'entertainment'...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573909</th>\n",
       "      <td>['well', 'shall', 'say', 'one', 'fun', 'rate',...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573910</th>\n",
       "      <td>['go', 'best', 'movie', 'ever', 'seen', 'seen'...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573911</th>\n",
       "      <td>['call', '1999', 'teenage', 'version', 'pulp',...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573912</th>\n",
       "      <td>['movie', 'made', 'doubt', 'sucker', 'family',...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>573913 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_review  is_spoiler\n",
       "0       ['oscar', 'year', 'shawshank', 'redemption', '...        True\n",
       "1       ['shawshank', 'redemption', 'without', 'doubt'...        True\n",
       "2       ['believe', 'film', 'best', 'story', 'ever', '...        True\n",
       "3       ['yes', 'spoiler', 'film', 'emotional', 'impac...        True\n",
       "4       ['heart', 'extraordinary', 'movie', 'brilliant...        True\n",
       "...                                                   ...         ...\n",
       "573908  ['go', 'wise', 'fast', 'pure', 'entertainment'...       False\n",
       "573909  ['well', 'shall', 'say', 'one', 'fun', 'rate',...       False\n",
       "573910  ['go', 'best', 'movie', 'ever', 'seen', 'seen'...       False\n",
       "573911  ['call', '1999', 'teenage', 'version', 'pulp',...       False\n",
       "573912  ['movie', 'made', 'doubt', 'sucker', 'family',...       False\n",
       "\n",
       "[573913 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CleanData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081c6130-e6bc-4022-b03c-c9fd2baaa543",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22bb9992-8855-4ca4-8b12-68517dd33229",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = CleanData['is_spoiler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc05a7cb-1bdd-4c54-bb9c-9ba871002ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData[\"clean_review\"] = CleanData[\"clean_review\"].apply(safe_literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91d6db5b-5f87-40b5-8dd5-358a2f274fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 573913 entries, 0 to 573912\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   clean_review  573913 non-null  object\n",
      " 1   is_spoiler    573913 non-null  bool  \n",
      "dtypes: bool(1), object(1)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "CleanData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec57fb1-b3c4-4bf1-9fe2-52efc41887c6",
   "metadata": {},
   "source": [
    "Create a dummy text thanks to the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9781a328-96b2-4ec3-a6a0-05df3400a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData[\"whole__text\"] = CleanData[\"clean_review\"].apply(join_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43b8ed58-016a-4669-b1cb-7ba41f3ffeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    oscar year shawshank redemption written direct...\n",
       "1    shawshank redemption without doubt one brillia...\n",
       "2    believe film best story ever told film tell ti...\n",
       "3    yes spoiler film emotional impact find hard wr...\n",
       "4    heart extraordinary movie brilliant indelible ...\n",
       "Name: whole__text, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = CleanData['whole__text']\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be68b2a-3188-4319-a9e0-ab093b7a193e",
   "metadata": {},
   "source": [
    "map the Boolean values in values 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "27b89b25-585b-4b06-82d8-3e589af797cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData['is_spoiler_numeric'] = np.where(CleanData['is_spoiler'] == True, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e181ddae-a300-4b00-b90f-e53a65fbb917",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData = CleanData.rename(columns={'is_spoiler_numeric': 'label','whole__text':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7687f4d3-b467-4b54-88e0-19a0edc957a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_train_test(CleanData, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6289031f-6aa0-4e8c-8d58-a14aca880b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['text','label']]\n",
    "test = test[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c962ed85-e197-4139-b8db-40e4c46ae49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 459130 entries, 94625 to 221631\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    459130 non-null  object\n",
      " 1   label   459130 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 10.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a68017-4915-449f-bf58-77d4e7d5fe84",
   "metadata": {},
   "source": [
    "### Transform the dataset\n",
    "\n",
    "Let's transform the pandas dataset into a TensorFlow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15dfbb86-c7fa-44d9-b607-3b9962f84ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(train['text'].values, tf.string),\n",
    "            tf.cast(train['label'].values, tf.int64)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "564d6cc7-a90d-42de-bfc4-cad08e827e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset =( \n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(test['text'].values, tf.string),\n",
    "            tf.cast(test['label'].values, tf.int64)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29e0215d-98b2-4c0b-992a-44f811a4ce18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "025b48dc-1ec8-4f3f-91c5-ab187eba6f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  b'maltese falcon film noir based novel title dashiell hammett directed john huston feature humphrey bogart private investigator sam spade mary astor femme fatale client gladys george peter lorre sydney greenstreet co star key supporting role story follows san francisco private detective dealing three unscrupulous adventurer competing obtain jewel encrusted falcon statuette sam spade hard boiled san francisco private eye unscrupulous next guy also adheres personal code honor office spade archer detective agency sweep miss wonderly offer large retainer sam partner mile archer protect someone named floyd thursby detective believe neither miss wonderly story believe money since archer saw first take case later evening shot death mysterious thursby miss wonderly real name turn brigid shaughnessey story continues sam also introduced effeminate joel cairo fat erudite kasper gutman turn brigid cairo gutman international scoundrel involved search foot high jewel encrusted statuette shape falcon though cairo gutman offer spade small fortune find black bird obviously willing commit mayhem murder towards goal gutman example drug spade allows gunsel wilmer kick beat unconscious detective suspenseful labyrinthine brilliantly cast maltese falcon one influential noirs film still tightest sharpest cynical hollywood official deathless classic bracingly tough even post tarantino standard among important influential movie emerge hollywood system significant way contemporary citizen kane action movie sort least implication character always seem keyed right verge erupting violence turning point picture several respect john huston made directorial debut bogart mostly played bad guy last minute substitution george raft must kicking year afterward role made bogart star established trend setting antihero persona classic movie becomes better multiple viewing indeed'\n",
      "label:  1\n"
     ]
    }
   ],
   "source": [
    "for example, label in training_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4272d-a6f6-44be-b03a-437109812dc2",
   "metadata": {},
   "source": [
    "## RNN MODEL\n",
    "Next, we define the model with its various layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2fc22521-aeb3-4a13-928b-95bec00d0435",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1e298-a033-4164-9d66-99c7e0178e2f",
   "metadata": {},
   "source": [
    "\n",
    "This code prepares the training and test datasets for training machine learning models using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa9e8092-eb33-4ae0-8c69-6dd78618a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = training_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc74bc77-e151-4e0e-8604-f345f05cad26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "493a7f86-bef9-44ee-834c-19e3cb740073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e8c41-1dc3-4b2a-88df-d8c1fb3c4bd4",
   "metadata": {},
   "source": [
    "\n",
    "Once adapted, the encoder can be used to convert textual input data (such as reviews, titles, or any other text) into numerical tensors that can be processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5ce79ff-4e90-4311-9f51-eeab074b6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(training_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f56b960-103e-4ce6-a94e-bde3c6700eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'movie', 'film', 'one', 'like', 'character', 'time',\n",
       "       'good', 'story', 'see', 'really', 'make', 'great', 'well', 'would',\n",
       "       'scene', 'get', 'even', 'much'], dtype='<U16')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b4256916-2a62-4c0c-9a16-b6071580c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=256,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "632b22c5-8805-4875-b4b7-9701f36c0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "             metrics=['accuracy',recall_m,precision_m,f1_m,false_negative_rate_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "073aa9a1-d404-439b-91e0-5163dfbea278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (Text  (None, None)              0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_15 (Embedding)    (None, None, 256)         2560000   \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirect  (None, 512)               1050624   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3742209 (14.28 MB)\n",
      "Trainable params: 3742209 (14.28 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334da22-1a4f-492c-9ee5-e61b0b58c330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  17/7174 [..............................] - ETA: 15:33 - loss: 0.6840 - accuracy: 0.7381 - recall_m: 0.0361 - precision_m: 0.0470 - f1_m: 0.0366 - false_negative_rate_m: 0.9639"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782a571-0f88-45cf-b845-ae13bda8e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy values for each epoch:\")\n",
    "for epoch in range(len(history.history['accuracy'])):\n",
    "    accuracy = history.history['accuracy'][epoch]\n",
    "    precision = history.history['precision_m'][epoch]\n",
    "    recall = history.history['recall_m'][epoch]\n",
    "    f1_score = history.history['f1_m'][epoch]\n",
    "    print(f\"Epoch {epoch}: {accuracy} \" f\"recall: {recall} \" f\"precision: {precision} \" f\"f1-score: {f1_score} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2fab53a3-303b-425a-9d30-b915bbaf4107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Media Training accuracy: 0.7775285243988037\n",
      "    Media Training precision: 0.662295420964559\n",
      "    Media Training recall: 0.35786786675453186\n",
      "    Media Training F1 score: 0.44650845726331073\n"
     ]
    }
   ],
   "source": [
    "training_accuracy_mean = np.mean(history.history['accuracy'])\n",
    "training_precision_mean = np.mean(history.history['precision_m'])\n",
    "training_recall_mean = np.mean(history.history['recall_m'])\n",
    "training_f1_score_mean = np.mean(history.history['f1_m'])\n",
    "\n",
    "print(f\"    Media Training accuracy: {training_accuracy_mean}\")\n",
    "print(f\"    Media Training precision: {training_precision_mean}\")\n",
    "print(f\"    Media Training recall: {training_recall_mean}\")\n",
    "print(f\"    Media Training F1 score: {training_f1_score_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e990d0c-263a-4792-aaa5-9aa7b8ae6ca2",
   "metadata": {},
   "source": [
    "### Result obtained\n",
    "Accuracy values for each epoch:\n",
    "Epoch 0: 0.7552915215492249 recall: 0.247200608253479 precision: 0.577804684638977 f1-score: 0.32188791036605835 \n",
    "Epoch 1: 0.7683684229850769 recall: 0.30710819363594055 precision: 0.664779543876648 f1-score: 0.40216726064682007 \n",
    "Epoch 2: 0.7724130153656006 recall: 0.315212219953537 precision: 0.6743733882904053 f1-score: 0.41411998867988586 \n",
    "\n",
    " Media Training accuracy: 0.7653576532999674\n",
    "    Media Training precision: 0.6389858722686768\n",
    "    Media Training recall: 0.28984034061431885\n",
    "    Media Training F1 score: 0.3793917198975881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "281b3aea-f7c9-45ab-ad37-c813e70aadef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1794/1794 [==============================] - 32s 17ms/step - loss: 0.4872 - accuracy: 0.7835 - recall_m: 0.5051 - precision_m: 0.5631 - f1_m: 0.5242 - false_negative_rate_m: 0.4949\n"
     ]
    }
   ],
   "source": [
    "resultTest=model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6f0fb31-f656-4f24-8e3b-002689b3348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputRNN.txt\", \"a\") as f:\n",
    "    print(f\"    Media Training accuracy: {training_accuracy_mean}\", file=f)\n",
    "    print(f\"    Media Training precision: {training_precision_mean}\", file=f)\n",
    "    print(f\"    Media Training recall: {training_recall_mean}\", file=f)\n",
    "    print(f\"    Media Training F1 score: {training_f1_score_mean}\", file=f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22b9b8ea-8aa3-4fad-b506-c2162c711d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputRNN.txt\", \"a\") as f:\n",
    "    print(\"Test Result\",file=f)\n",
    "    print(f\"  Loss: {resultTest[0]}, Accuracy: {resultTest[1]}, F1: {resultTest[4]}, Precision: {resultTest[3]}, Recall: {resultTest[2]}\",file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6b36578-aa0f-475e-a551-09dc6ba97ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss: 0.4872026741504669, Accuracy: 0.7834609746932983, F1: 0.5242252945899963, Precision: 0.5631215572357178, Recall: 0.5051186084747314\n"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080cdfa0-c8bc-40e8-82e0-a3be0c1728ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Eval Loss: {log_history['eval_loss']}, Accuracy: {log_history['eval_accuracy']}, F1: {log_history['eval_f1']}, Precision: {log_history['eval_precision']}, Recall: {log_history['eval_recall']}\",file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9bd8913-81a7-41ab-b3a5-b046a2745909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4872026741504669, 0.7834609746932983, 0.5051186084747314, 0.5631215572357178, 0.5242252945899963, 0.49488136172294617]\n"
     ]
    }
   ],
   "source": [
    "resultTest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
